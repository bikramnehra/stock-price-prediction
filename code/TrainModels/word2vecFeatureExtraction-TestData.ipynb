{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    import string\n",
    "    from nltk.corpus import stopwords\n",
    "    import datetime\n",
    "    from pyspark.sql.types import StructType,StructField,StringType,DateType,FloatType,IntegerType,ArrayType\n",
    "    from pyspark.sql import SQLContext\n",
    "    from pyspark.sql.functions import udf\n",
    "    import copy\n",
    "    from pyspark.mllib.linalg import DenseVector\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def flatten((k,v)):\n",
    "        res = []\n",
    "        for item in v:\n",
    "            res.append((k,item))\n",
    "        return res\n",
    "\n",
    "    def getPreProcessedWords(s):\n",
    "        for punc in string.punctuation:\n",
    "            s = s.replace(punc,' ')\n",
    "        words = s.lower().split()\n",
    "        ret = []\n",
    "        for w in words:\n",
    "            if w not in stop_words:\n",
    "                ret.append(w)\n",
    "        return ret\n",
    "\n",
    "    wordVecDict = dict(sc.pickleFile('./wordFeatures').collect())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def mapFeatures((k,v)):\n",
    "        res = []\n",
    "        for word in v:\n",
    "            if(word in  wordVecDict):\n",
    "                res.append(wordVecDict[word])\n",
    "        if len(res) != 0:\n",
    "            return (k,(1,sum(res)/len(res)))\n",
    "        else:\n",
    "            return (k,(1,DenseVector([0]*100)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sc.pickleFile('./TrainNews').\\\n",
    "            filter(lambda (k,v):k[1].year == 2014).\\\n",
    "            union(sc.pickleFile('./TestNews')).\\\n",
    "            flatMap(flatten).\\\n",
    "            map(lambda (k,v):(k,getPreProcessedWords(v[1]))).cache()\n",
    "#filter(lambda (k,v):k[0]==\"AAPL\").\\\n",
    "            \n",
    "wordVecFeatures = data.map(mapFeatures).\\\n",
    "                    reduceByKey(lambda c1,c2:(c1[0]+c2[0],c1[1]+c2[1])).\\\n",
    "                    cache()\n",
    "\n",
    "#wordVecFeatures.count(),sc.pickleFile('./TrainNews').count()\n",
    "#sc.pickleFile('../datadownload/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,DateType,FloatType,IntegerType,ArrayType\n",
    "import datetime\n",
    "\n",
    "window = 30\n",
    "def mapDates(v):\n",
    "    l = k.split(\"-\")\n",
    "    d = datetime.datetime.strptime(\"-\".join(l[1:]),\"%Y-%m-%d\")\n",
    "    ret = {}\n",
    "    ret['Symbol'] = l[0]\n",
    "    ret['Date'] = d\n",
    "    ret['PercentMovement'] = v[0]\n",
    "    ret['Price'] = v[1]\n",
    "    ret['DayId'] = v[2]\n",
    "    ret['Direction'] = v[3]\n",
    "    return ret\n",
    "\n",
    "schema = StructType([   StructField(\"Symbol\",StringType()),\n",
    "                        StructField(\"Date\",DateType()),\n",
    "                        StructField(\"PercentMovement\",FloatType()),\n",
    "                        StructField(\"Price\",FloatType()),\n",
    "                        StructField(\"DayId\",IntegerType()),\n",
    "                        StructField(\"Direction\",StringType())])\n",
    "\n",
    "\n",
    "rdd = sc.pickleFile('./LabelsTrain').\\\n",
    "        filter(lambda l:l['Date'].year == 2014).\\\n",
    "        union(sc.pickleFile('./LabelsTest')).cache()\n",
    "        #filter(lambda l:l['Symbol'] == 'AAPL').cache()\n",
    "\n",
    "rddKey = rdd.map(lambda l:((l['Symbol'],l['DayId']),l))\n",
    "\n",
    "def flatMapRange(l):\n",
    "    r = range(l['DayId']+1,l['DayId']+window+1)\n",
    "    ret =  []\n",
    "    for i in r:\n",
    "        ret.append(((l['Symbol'],i),l))\n",
    "    return ret\n",
    "\n",
    "def mapAggKey((k,v)):\n",
    "    act = v[0]\n",
    "    act['Last-T'] = [v[1]]\n",
    "    return ((act['Symbol'],act['DayId']),act)\n",
    "\n",
    "def reduceAggKey(v1,v2):\n",
    "    v1['Last-T'] = v1['Last-T'] + v2['Last-T']\n",
    "    return v1\n",
    "\n",
    "def sortAggKey((k,v)):\n",
    "    v['Last-T'] = sorted(v['Last-T'],key = lambda l:l['DayId'])\n",
    "    return v\n",
    "\n",
    "def getDateRange(start,end):\n",
    "    ret = []\n",
    "    while start < end:\n",
    "        ret.append(start)\n",
    "        start = start + datetime.timedelta(days=1)\n",
    "    return ret\n",
    "    #break\n",
    "\n",
    "def mapRevDate(di):\n",
    "    res = []\n",
    "    childs = di['Last-T']\n",
    "    di.pop('Last-T')\n",
    "    #childs[0]['Date']['Parent'] = di\n",
    "    childs[0]['Parent'] = di\n",
    "    for da in  getDateRange(childs[0]['Date'],childs[0]['Parent']['Date']):\n",
    "            res.append(((childs[0]['Symbol'],da),childs[0]))\n",
    "    for d in range(1,len(childs)):\n",
    "        #d['Parent'] = di\n",
    "        childs[d]['Parent'] = di\n",
    "        for da in  getDateRange(childs[d]['Date'],childs[d-1]['Date']):\n",
    "            res.append(((childs[d]['Symbol'],da),childs[d]))\n",
    "    return res\n",
    "\n",
    "TestSet = [2015,2016]\n",
    "\n",
    "rddRange = rdd.flatMap(flatMapRange).\\\n",
    "            join(rddKey).\\\n",
    "            map(mapAggKey).\\\n",
    "            reduceByKey(reduceAggKey).\\\n",
    "            map(sortAggKey).\\\n",
    "            flatMap(mapRevDate).\\\n",
    "            filter(lambda (k,v):k[1].year in TestSet).\\\n",
    "            cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'AAPL', datetime.datetime(2015, 5, 20, 0, 0)),\n",
       "  {'AClose': 128.226289,\n",
       "   'Close': 130.059998,\n",
       "   'Date': datetime.datetime(2015, 5, 20, 0, 0),\n",
       "   'DayId': 177,\n",
       "   'Dir': 'st',\n",
       "   'High': 130.979996,\n",
       "   'Low': 129.339996,\n",
       "   'Open': 130.0,\n",
       "   'Parent': {'AClose': 129.537539,\n",
       "    'Close': 131.389999,\n",
       "    'Date': datetime.datetime(2015, 5, 21, 0, 0),\n",
       "    'DayId': 176,\n",
       "    'Dir': 'up',\n",
       "    'High': 131.630005,\n",
       "    'Low': 129.830002,\n",
       "    'Open': 130.070007,\n",
       "    'PerMov': 1.0226062145493433,\n",
       "    'Symbol': u'AAPL',\n",
       "    'Volume': 39730400.0},\n",
       "   'PerMov': -0.007695957328962233,\n",
       "   'Symbol': u'AAPL',\n",
       "   'Volume': 36454900.0})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddRange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mapWord2VecJoin((k,(di,wo))):\n",
    "    di['WordVec'] = wo\n",
    "    key = (di['Symbol'],di['Date'],di['Parent']['Date'])\n",
    "    return (key,di)\n",
    "\n",
    "def reduceWord2VecJoin(d1,d2):\n",
    "    c = d1['WordVec'][0] + d2['WordVec'][0]\n",
    "    vec = d1['WordVec'][1] + d2['WordVec'][1]\n",
    "    d1['WordVec'] = (c,vec)\n",
    "    return d1\n",
    "\n",
    "def rddRangeJoinAvg((k,d)):\n",
    "    avg = d['WordVec'][1] / d['WordVec'][0]\n",
    "    parent = d['Parent']\n",
    "    d['WordVec'] = avg\n",
    "    d.pop('Parent')\n",
    "    parent['Child'] = [d]\n",
    "    key = (parent['Symbol'],parent['Date'])\n",
    "    return (key,parent)\n",
    "\n",
    "def reduceAppendChild(d1,d2):\n",
    "    d1['Child'] = d1['Child'] + d2['Child']\n",
    "    return d1\n",
    "\n",
    "def sortChildren((k,d)):\n",
    "    d['Child'] = sorted(d['Child'],key=lambda l:l['DayId'])\n",
    "    return (d)\n",
    "\n",
    "finalData = rddRange.join(wordVecFeatures).\\\n",
    "        map(mapWord2VecJoin).\\\n",
    "        reduceByKey(reduceWord2VecJoin).\\\n",
    "        map(rddRangeJoinAvg).\\\n",
    "        reduceByKey(reduceAppendChild).\\\n",
    "        map(sortChildren).cache()\n",
    "        \n",
    "#finalData.saveAsPickleFile('FeatureVector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finalData.saveAsPickleFile('FeatureVectorTest')#sortBy(lambda (k,(v1)):v1['Parent']['Date']).take(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
